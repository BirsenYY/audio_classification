{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c49b05-0b3f-4a70-a018-d54b5edddc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with hyperparameter tuning\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from advanced_model import AdvancedAudioClassifier\n",
    "import wandb\n",
    "import optuna\n",
    "\n",
    "# Directory where the preprocessed data is stored\n",
    "data_dir = 'preprocessed_data'\n",
    "\n",
    "def load_data_for_fold(fold, train=True):\n",
    "    \"\"\"\n",
    "    Load spectrograms and labels for a specific fold.\n",
    "\n",
    "    Args:\n",
    "    - fold (int): The fold number to load.\n",
    "    - train (bool): Whether to load the training or validation set. Default is True (training set).\n",
    "\n",
    "    Returns:\n",
    "    - spectrograms (Tensor): Loaded spectrograms.\n",
    "    - labels (Tensor): Corresponding labels.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        spectrograms = torch.load(os.path.join(f'{data_dir}/fold{fold}', 'spectrograms_augmented.pt'))\n",
    "    else:\n",
    "        spectrograms = torch.load(os.path.join(f'{data_dir}/fold{fold}', 'spectrograms.pt'))\n",
    "    \n",
    "    labels = torch.tensor(torch.load(os.path.join(f'{data_dir}/fold{fold}', 'labels.pt')), dtype=torch.long)\n",
    "    return spectrograms, labels\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The model to evaluate.\n",
    "    - val_loader (DataLoader): DataLoader for the validation set.\n",
    "    - criterion (Loss): The loss function.\n",
    "\n",
    "    Returns:\n",
    "    - val_loss (float): Average loss on the validation set.\n",
    "    - val_accuracy (float): Accuracy on the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            \n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)  # Get the index of the max log-probability\n",
    "            total += labels.size(0)  # Update the total number of samples\n",
    "            correct += predicted.eq(labels).sum().item()  # Update the number of correct predictions\n",
    "\n",
    "    val_loss = running_loss / total  # Compute average loss over all samples\n",
    "    val_accuracy = correct / total  # Compute accuracy\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def train_model(trial, train_loader, val_loader, num_epochs, model, criterion, optimizer, scheduler):\n",
    "    \"\"\"\n",
    "    Train the model and evaluate it on the validation set at each epoch.\n",
    "\n",
    "    Args:\n",
    "    - trial (optuna.Trial): Optuna trial object for hyperparameter optimization.\n",
    "    - train_loader (DataLoader): DataLoader for the training set.\n",
    "    - val_loader (DataLoader): DataLoader for the validation set.\n",
    "    - num_epochs (int): Number of epochs to train.\n",
    "    - model (nn.Module): The model to train.\n",
    "    - criterion (Loss): The loss function.\n",
    "    - optimizer (Optimizer): The optimizer.\n",
    "    - scheduler (LRScheduler): Learning rate scheduler.\n",
    "\n",
    "    Returns:\n",
    "    - val_accuracy (float): Validation accuracy after the final epoch.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            \n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            \n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_accuracy = correct / total\n",
    "        \n",
    "        # Log training progress to wandb\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "        })\n",
    "\n",
    "        # Evaluate the model\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        # Log validation progress to wandb\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')\n",
    "\n",
    "        scheduler.step(val_loss)  # Adjust learning rate based on validation loss\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "    - trial (optuna.Trial): Optuna trial object.\n",
    "\n",
    "    Returns:\n",
    "    - mean_val_accuracy (float): Mean validation accuracy across folds.\n",
    "    \"\"\"\n",
    "    all_val_accuracy = []\n",
    "    # Sample hyperparameters\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-3)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 512])  \n",
    "\n",
    "    num_epochs = 15\n",
    "    \n",
    "    for fold in range(1, 2):\n",
    "        X_val, y_val = load_data_for_fold(fold, train=False)\n",
    "        X_train, y_train = [], []\n",
    "        for train_fold in range(1, 11):\n",
    "            if train_fold == fold:\n",
    "                continue\n",
    "            X_fold, y_fold = load_data_for_fold(train_fold, train=True)\n",
    "            X_train.extend(X_fold)\n",
    "            y_train.extend(y_fold)\n",
    "\n",
    "        X_train = torch.stack(X_train)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(torch.stack(X_val), y_val)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = AdvancedAudioClassifier(dropout_rate).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "        \n",
    "        val_accuracy = train_model(trial, train_loader, val_loader, num_epochs, model, criterion, optimizer, scheduler)\n",
    "        all_val_accuracy.append(val_accuracy)\n",
    "    \n",
    "    return np.mean(all_val_accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize wandb for logging\n",
    "    wandb.init(project=\"audio_classification\", entity=\"username\")\n",
    "\n",
    "    # Determine the device to use (MPS, CUDA, or CPU)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create an Optuna study for hyperparameter optimization\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    print(\"Best hyperparameters: \", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bca41-a96c-4a03-9324-9a798be97e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e504e-64d6-4c42-b115-5fd74a3b0c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
